The performance evaluation is split into two: a shared memory performance
scaling evaluation on a 64 core machine, and a distributed memory performance
evaluation on a cluster on up to 12 nodes.

\subsection{Evaluation Methodology}
\label{sec:mpievaluationmethodology}

The performance evaluation aimed to test the scalability of the parallelised
application in two ways. The first is with a fixed area under test. This will
show how a given problem (150x150x90 in size) would scale in multi-core and
multi-socket machines and in a cluster. The second test is with an expanding
area. This is where, for all runs, each process has its own area of a set size
(150x150x90) such that, as the number of processes grows, the overall
computational area increases at the same rate. This will show how well larger
problem sets can be handled and how the overhead of communication grows as the
number of processes involved in the communication grows.

For a given number of processes, there can be many runs. This is because there
are different ways to lay out the processes in a two dimensional grid. For
instance, the 64 process runs are executed in a 4x16, 8x8, and 16x4 layout. The
quickest layout is reported as the runtime for that number of processes. To
limit the number of configurations, the maximum number of processes in any
single dimension was limited to 16. This limit is for the evaluation only, the
LES supports arbitrarily large process grids.

Many of the graphs, for instance Figure~\ref{fig:mpichfixedarea}, have the
occasional spike in runtime. These occur at a prime number of processes. A run
with a prime number of processes will have to have a 1xN or Nx1 layout over the
LES area. This limits the number of neighbours per process to two but, rather
than improving performance due to the reduced number of messages, performance
degrades. This is due to the non-blocking nature of the sends and receives used
in the MPI LES. For other process counts, with up to four neighbours, there is
less work to do for each message exchange meaning the first exchange will start
sooner and have more overlap with subsequent exchanges than the prime number
cases.

\subsection{Shared-memory parallelisation}
\input{includes/mpilesevalshared.tex}

\subsection{Distributed-memory paralellism}
\input{includes/mpilesevaldistributed.tex}

\subsection{Estimated versus exact corners}

The majority of the communication in the LES involves halo exchanges. These
involve sending and receiving up to four edges between neighbouring processes.
To be mathematically correct, there would also need to be up to four other
communications, a small number of values need to be sent and received for corner
values from four other processes. This brings the total number of messages per
process up to eight. This was thought to be a potential source of overhead,
especially given the overhead of sending a single message was likely to outweigh
the transfers itself. To test this, all runs were repeated with corners instead
being calculated rather than exchanged.

For the shared memory runs, there was a negligible change in performance. This
is likely to be because the cost of calculating floating point units is very
similar to simple memory copies between memory spaces, especially in an AMD
system where floating point units are shared between pairs of cores in what AMD
calls a module.

For the distributed memory runs, exact corners were up to 5\% faster than the
estimated corner runs. The improvement is somewhat more surprising than the
shared memory case however given the fast Infiniband Interconnect between nodes,
the cost of sending a message is likely to be only marginally higher than
communication within a single node. The performance improvement is likely due to
the Xeon X5650's older architecture meaning it supports fewer instruction set
extensions such as AVX and FMA that are known to improve integer and floating
point operation performance.
