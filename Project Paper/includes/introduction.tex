The ecosystem of our planet is highly complex. Climate scientists wanting to
model parts of the ecosystem create independent and computationally-intensive
simulations for each individual aspect; for example, ocean, wind, land, and
atmosphere. One such simulation is the Large Eddy Simulation (LES) developed by
Hiromasa Nakayama and Haruyasu Nagai at the Japan Atomic Energy Agency and
Professor Tetsuya Takemi at the Disaster Prevention Research Institute of Kyoto
University \cite{Nakayama2011,Nakayama2012}. The LES is used to model turbulent
air flows for the study of urban boundary-layer flows in computational fluid
dynamics (CFD) calculations under realistic meteorological conditions. The LES
is implemented in FORTRAN 77 and is single threaded.

With multi-core processors and multi-socket motherboards widely available for a
single machine, it is beneficial to parallelise the LES to make better use of
the computational power of a shared memory system. Also, with computer clusters
commonplace in the area of research simulation, a parallelised implementation of
the LES that can make use of a distributed memory system is highly desirable.
The parallelisation can reduce the execution time of a single execution run or
allow higher resolution simulations to be run in the same timeframe as the
original single threaded implementation.

Work by Wim Vanderbauwhede has seen the LES automatically ported to Fortran 95
and an OpenCL implementation has been created
\footnote{\url{http://github.com/wimvanderbauwhede/LES}}. While this has been
shown to improve performance on shared memory systems and facilitates the use of
heterogeneous systems such as those with GPUs, the OpenCL-accelerated LES may
not offer the best possible performance on a CPU and it is unable to benefit
from distributed memory systems.

This paper presents two implementations of the LES. The first makes use of the
industry standard for distributed memory parallelism, MPI, and the second is
implemented using a new framework by Wim Vanderbauwhede, the Glasgow Model
Coupling Framework (GMCF). The GMCF implementation is capable of shared memory
parallelism only as the framework is still in the early stages of development.
Future work for the framework includes the ability to have distributed memory
parallelism along with automated model coupling and parallelism.

The major contributions of the work discussed in this paper are as follows:

\begin{itemize}[noitemsep,nolistsep]

    \item MPI implementation of the LES for shared-memory and distributed-memory
    parallelism

    \item Performance evaluation of MPI LES

    \item GMCF implementation of the LES for shared-memory parallelism

    \item Performance evaluation of GMCF LES

    \item GMCF performance investigation and framework improvements

\end{itemize}

The rest of the paper is structured as follows: Section~\ref{sec:Background}
discusses the background of GMCF. Section~\ref{sec:MPILES} discusses the MPI
implementation of the LES with Section~\ref{sec:MPILESEval} conducting a
shared-memory and distributed-memory performance evaluation.
Section~\ref{sec:GMCFLES} then discusses the GMCF implementation including the
framework changes required to facilitate model parallelisation with
Section~\ref{sec:GMCFLESEval} conducting a shared-memory performance evaluation.
Section~\ref{sec:GMCFInvestigation} conducts an in-depth performance
investigation of the GMCF framework while outlining the performance-related
improvements implemented. Section~\ref{sec:FutureWork} discusses the future work
to improve the capabilities and performance of the GMCF framework with
Section~\ref{sec:Conclusion} concluding the paper.
