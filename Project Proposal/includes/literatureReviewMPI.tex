
Message Passing Interface (MPI) is a recognised standard for sharing data
between processes on parallel systems by supporting message passing between
nodes in a computer cluster or supercomputer. The standard creates a common
interface that allows any vendor to create their own implementation and ensures
that different implementations of the same version of the standard can be
interchanged without requiring user code changes for the most part. This has led
MPI to be viewed as the \textit{de facto} standard for inter-process
communication, especially across multiple nodes. There are implementations of
the standard for many languages including C, Fortran, Java, and Python.

\subsubsection{Multithreaded MPI Communication}

In addition to computer systems making use of multiple machines, each individual
node now has multiple CPU cores and sometimes even multiple CPU sockets. This
poses an additional problem for application developers: how to enable the use of
MPI inside a multithreaded application? Multiple threads and MPI individually
bring their own complexity that a developer has to deal with however there is an
additional challenge when considering how multiple threads make MPI calls.

D\'{o}zsa and Kuma et al.\ \cite{Kumar} use multichannel-enabled network
hardware, granular locking, atomic operations, and concurrency-aware message
queues to demonstrate that multithreaded applications can have a message passing
rate that scales with multiple threads. The authors send zero-byte messages to
measure the number of messages that can be sent per second as the number of
threads grows. There is a near-linear increase in performance as the number of
threads is increased whereas the default implementation sees a message rate
\textit{decrease} as thread count increases. Performance is still worse than the
Deep Computing Message Framework (DCMF) which is a lower level messaging library
and has a message rate that scales almost linearly with the number of threads
and has performance nearly seven times greater than the optimised MPI
implementation when both run on four threads. The authors haven't considered the
latency of messages or raw throughput since there may be an effect of the size
of messages in a threaded environment and zero-byte messages are unrealistic.
The authors have only investigated message rate for a small range of threads (1
to 4) however there can be single nodes with 64 cores (16 core CPUs on a
quad-socket motherboard) or more which is where any multithreading-related
performance problems would come to light.

Thakur and Gropp \cite{Thakur2009} create and use a number of performance
benchmarks using scenarios claimed to be close to typical applications. They
test the cost of thread safety, the ability for concurrent progress to be made,
and the possibilities for computation overlap. The authors' first test involves
a simple ping-pong latency test of a single threaded application that sets up
the MPI environment to allow multiple threads to make MPI calls without having
to explicitly synchronise with the other threads. The test shows negligible
latency on some MPI implementations. The validity of this test can be questioned
because, even though the MPI library is told to expect multiple threads making
calls simultaneously, this doesn't happen since the processes remain single
threaded. This means that the effect of any locks etc used by the MPI library
may not be being appropriately exercised to determine the overhead. The authors'
second test investigated cumulative bandwidth of MPICH2 and Open MPI
implementations, comparing each library's performance where parallelism is
achieved via processes or threads. For the Linux cluster the 1Gbit/s link
between nodes is saturated in all bar one case and thus no conclusions can be
made. For the Sun and IBM cases, threading is shown to dramatically decrease
throughput (over 50\%) in the threaded case compared to the process parallel
case. The bandwidth in Sun's multithreaded case was less than the saturated
bandwidth multithreaded case for the Linux cluster so this appears to show that
Sun can significantly improve the performance of their MPI implementation.

The other tests overall show how MPICH2 and Open MPI have fairly good
performance however the authors note that, due to the Gigabit Ethernet
interconnect, the overheads are generally masked since the Ethernet connection
is saturated in all cases, as shown by the cumulative bandwidth test. The paper
has a more comprehensive look into the performance differences of MPI libraries
and shows that significant work is required to minimise the overhead caused by
multithreaded MPI applications. The authors also make their benchmark test suite
available online for others to repeat their tests and see how different hardware
setups react and how later versions of MPI libraries improve.
