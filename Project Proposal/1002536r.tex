\documentclass{acm_proc_article-sp}

\usepackage{hyperref}

\title{Coupling the distributed Large Eddy Simulation and Weather Research and
Forecasting model using OASIS3-MCT and MPI}

\numberofauthors{1}

\author{
    \alignauthor
    Gordon Reid\\
    \affaddr{School of Computing Science}\\
    \affaddr{University of Glasgow}\\
    \email{1002536r@student.gla.ac.uk}
}

\let\underscore\_
\renewcommand{\_}{\underscore\hspace{0pt}}

\begin{document}

\maketitle

\begin{abstract}

Scientists studying the geosciences have created software which models different
aspects of our planet's ecosystem, for example ocean, wind, land and atmosphere.
On their own, each model is useful however for higher accuracy, and to model
more complex aspects of our climate, these models need to be coupled together.
The project proposed involves creating a distributed version of the Large Eddy
Simulator (LES) then coupling the LES with the Weather Research and Forecasting
model (WRF). The coupled system should scale to large computing clusters.
Ideally the coupling would also be generic such that the OpenCL-accelerated LES
could be used to allow greater performance on heterogeneous systems.

\end{abstract}

\section*{Introduction}

Our planet's ecosystem is highly complex. Geoscientists that are interested in
modelling this ecosystem have created numerous systems which each model one
aspect, for instance a Large Eddy Simulator (LES) to study turbulent air flows
\cite{Nakayama2011,Nakayama2012} and the Weather Research and Forecasting Model
(WRF) for mesoscale weather prediction. For the last decade or so, interest has
evolved from individual models to combining models \cite{Michalakes2010}. With
this trend, a number of model coupling frameworks have been created to support
the desire for co-simulation. These allow a single application to make use of
multiple models without significant changes to the individual models' code
bases. Models communicate with each other via data exchanges at each timestep
which are handled by the coupling framework. The coupling frameworks typically
use the Message Passing Interface (MPI) for communication since MPI is the
\textit{de facto} standard for parallel communication.

The proposed work for the project involves two distinct stages. The first
requires the modification of existing LES code to create a distributed version
using MPI for use on computer clusters. An additional aim is to allow use of the
OpenCL-accelerated LES \cite{Vanderbauwhede2014} in the distributed system to
improve the model's throughput further. The second stage involves the coupling
of the distributed LES with the WRF using a model coupling framework. There is a
potential third stage which involves comparing the coupling from the second
stage to a coupling using the Glasgow Model Coupling Framework (GCMF)
\cite{Vanderbauwhede2014}.

A number of modeling frameworks have been looked at, including: the Model
Coupling Toolkit (MCT) \cite{Larson2005,Jacob2005}; OASIS3-MCT
\cite{Valcke,Valcke2013}; the Earth System Modeling Framework (ESMF)
\cite{Ramework2004}; and OpenPALM \cite{Piacentini2011}. After comparing these
modeling frameworks, OASIS3-MCT was chosen given its continued development and
simple coupling mechanism.

The remainder of the document discusses the research problem in detail before
going onto an in-depth literature review for the state of the art in distributed
and coupled applications. The proposed approach and work plan for the project is
then given to conclude the proposal.

\section*{Statement of Problem}

Individuals models for ocean, land, atmosphere, etc are useful in ther own right
however geoscientists are now wanting to model more complex scenarios that
require collaboration of multiple models. The collaboration can also lead to
more accurate results since additional effects of other parts of the system can
be added to the model in question e.g. coupling WRF and LES led to greater wind
velocity being predicted compared with running WRF on its own
\cite{Kinbara2010,Nakayama1998}.

Currently, WRF and LES are independent models and LES has two variants: the
original single threaded code and the OpenCL accelerated variant
\cite{Vanderbauwhede2014}. Vanderbauwhede and Takemi \cite{Vanderbauwhede2013}
have also investigated the benefits of GPU accelerating WRF and found this to be
``feasible and worthwhile''.

The problem to be addressed is two-fold: creating a MPI variant of LES to allow
LES to run on multiple nodes in a computer cluster and then coupling this
variant of LES with WRF. The MPI variant of the LES will increase the
performance of the simulator since it will allow it to make use of a multi-node
distributed memory system such as a Beowulf cluster. The coupling of MPI LES
with WRF will create a system that benefits from scalable performance and high
accuracy results.

\section*{Literature Review}

Previous work falls under two related but independent areas: distributed
computing using MPI for data sharing, and model coupling. Model couplers
generally use MPI for cross-model communication however this is generally hidden
to the user under an API offered by the model coupling framework. MPI specific
work will be discussed followed by a section dedicated to related work on model
coupling.

\subsection*{Message Passing Interface}

Message Passing Interface (MPI) is a recognised standard for sharing data
between processes on parallel systems and supports message passing between nodes
in a computer cluster or supercomputer. The standard creates a common interface
that allows anybody to create their own implementation and ensures that
different implementations of the same version of the standard can be
interchanged without requiring user code changes. This has led MPI to be a
\textit{de facto} standard for inter-process communication, especially across
multiple nodes. There are implementations of the standard for many languages
including C, Fortran, Java, and Python.

\subsubsection*{Multithreaded MPI Communication}

In addition to computer systems making use of multiple machines, each individual
node now has multiple CPU cores and multiple CPU sockets. This poses an
additional problem for application developers: how to enable the use of MPI
inside a multithreaded application? Multiple threads and MPI individually bring
their own complexity that a developer has to deal with however there is an
additional challenge when considering how multiple threads make MPI calls. The
concurrent calling of MPI functions can pose issues especially if the data being
sent or received is shared by multiple threads. The MPI standard defines four
levels of thread support: \textbf{MPI\_THREAD\_SINGLE} requiring user code to be
single threaded; \textbf{MPI\_THREAD\_FUNNELED} allowing user code to be
multithreaded but one thread makes MPI calls; \textbf{MPI\_THREAD\_SERIALIZED}
allowing user code to be multithreaded but MPI library calls are serialised and
\textbf{MPI\_THREAD\_MULTIPLE} where threads can make MPI calls without
concerning themselves about the other threads. Thakur and Gropp
\cite{Thakur2009} and D\'{o}zsa and Kuma et al.\ \cite{Kumar} have discussed the
challenges of code correctness and performance of multithreaded MPI
communication however they only focus on the \textbf{MPI\_THREAD\_MULTIPLE}
case. This limitation isn't necessarily significant given that this is the most
general case with \textbf{MPI\_THREAD\_FUNNELLED} and
\textbf{MPI\_THREAD\_SERIALIZED} being restricted cases of multithread MPI
programs.

D\'{o}zsa and Kuma et al.\ \cite{Kumar} use a multichannel-enabled network
hardware, granular locking, atomic operations, and concurrency-aware message
queues to demonstrate that multithreaded applications can have a message passing
rate that scales with multiple threads. The authors send zero-byte messages to
measure the number of messages that can be sent per second as the number of
threads grows. There is a near-linear increase in performance as the number of
threads is increased whereas the default implementation sees a message rate
\textit{decrease} as thread count increases. Performance is still worse than the
Deep Computing Message Framework (DCMF) which is a lower level messaging library
and has a message rate that scales almost linearly with the number of threads
and has performance nearly seven times greater than the optimised implementation
when both run on four threads. The authors also haven't considered latency of
messages or raw throughput since there may be an effect of the size of messages
in a threaded environment and zero-byte messages are unrealistic. The authors
have only investigated message rate for a small range of threads (1 to 4)
however there can be single nodes with up to 64 cores (16 core CPUs on a
quad-socket motherboard) which is where any multithreading-related performance
problems would come to light.

Thakur and Gropp \cite{Thakur2009} create and use a number of performance
benchmarks using scenarios claimed to be close to typical applications. They
test the cost of thread safety, the ability for concurrent progress to be made,
and the possibilities for computation overlap. The authors first test involves a
simple ping-pong latency test of a single threaded application that calls
\textbf{MPI\_Init\_thread} with \textbf{MPI\_THREAD\_MULTIPLE}. The test shows
negligible latency on some MPI implementations. The validity of this test can be
question because, even though the MPI library is told to expect multiple threads
making calls simultaneously, this doesn't happen and thus the effect of locks
etc may not be being appropriately exercised to determine the overhead. The
authors second test investigated cumulative bandwidth of MPICH2 and Open MPI
implementations, comparing each libraries performance where parallelism is
achieved via processes or threads. For the Linux cluster, performance in all bar
one case, the 1Gbit/s link between nodes is saturated and thus no conclusions
can be made. For the Sun and IBM cases, threading is shown to dramatically
decrease throughput (over 50\%) in the threaded case compared to the process
parallel case. The bandwidth in Sun's multithreaded case was less than the
saturated bandwidth multithreaded case for the Linux cluster so this appears to
show that Sun can significantly improve the performance of their MPI
implementation.

The other tests overall show how MPICH2 and Open MPI have fairly good
performance however the authors note that, due to the Gigabit Ethernet
interconnect, the overheads are generally masked since the ethernet connection
is saturated in all cases, as shown by the cumulative bandwidth test. The paper
has a more comprehensive look into the performance differences of MPI libraries
and shows that significant work is required to minimise the overhead caused by
multithreaded MPI applications. The authors also make their benchmark test suite
available online for others to repeat their tests and see how different hardware
setups react and how later versions of MPI libraries improve.

\subsection*{Model Couplers}

Applications currently exist that model a single aspect of our Earth's climate.
Each model is useful in their own right however more accurate and detailed
analysis can be conducted by making use of multiple models. For instance, the
effect ocean and land temperatures have on airflows could be investigated. This
isn't viable by using the models independently so there is significant
motivation to construct an efficient method of allowing applications to
interact.

There are three ways applications can be coupled together \cite{Thevenin}. The
first involves merging the two code bases. This solution is very efficient and
portable as a single process is compiled. Data is shared via memory exchanges
which are very lightweight however the process of merging is very complicated
and the two code bases are no longer independent making continued development of
each model difficult. Merging also causes problems for parallelism since each
model's parallel abilities are dictated by the overarching code that controls
both models. For these reasons this solution is not an appropriate way to couple
models.

The second solution \cite{Thevenin} is to use a communication library directly,
such as MPI. This allows each model to be kept independent with the only code
changes involved being related to the communication between each model itself.
The solution still suffers from many problems. The coupling code isn't generic
since each model needs to know exactly what model it is sending data to. Also,
since the MPI API is fairly low level, knowledge of parallel computing
techniques is required for efficient coupling. The problems of complexity and
genericness increase if there are many models involved in the coupling with
complicated exchanges required. Using MPI for data exchanges is definitely a
viable way forward however a higher level solution is required to make coupling
many models viable.

The third solution \cite{Thevenin} follows on from the second. Instead of using
an MPI library directly, a coupling library should be employed to extract the
low level complexities of coupling. A coupling library is a separate piece of
software that acts as the ``middle man'' between models and facilitates data
exchanges between models using a generic interface. Coupling libraries can also
provide tools for interpolation such as re-gridding if there is a difference
between data representations between communicating models. Some coupling
libraries also supply tools for performance analysis with graphical
representations of runtime execution characteristics. There are many examples of
coupling libraries in use, for example: the Model Coupling Toolkit (MCT)
\cite{Larson2005,Jacob2005}; OASIS3-MCT \cite{Valcke,Valcke2013}; the Earth
System Modeling Framework (ESMF) \cite{Ramework2004}; and OpenPALM
\cite{Piacentini2011}.

There are two main types of coupling: static and dynamic
\cite{Thevenin,Piacentini2011}. In static coupling, all of the coupled
components have to start simultaneously at the beginning of the simulation. This
means any memory and CPU resources required by each model are allocated and
locked until the end of the simulation, even if a model doesn't start using the
resources until midway through the simulation. Dynamic coupling allows each
coupled component to start and end as required, freeing up resources when not in
use. Instead, resources are managed by the coupler. Generally there is a fixed
amount of resources available to the coupler at the beginning of the simulation
however the coupler is free to dynamically manage how the resources are used to
meet the requirements of the currently running components.

During runtime, the models themselves can either be run sequentially or
concurrently \cite{Maisonnave}. If two models need the results of each other
from a previous timestep, they can run concurrently. If one of the models needs
the result of the other from the current time step, the models run sequentially.
If more than two models are coupled together, both sequential and concurrent
execution may be required.

Coupling parallel models has a problem that is generic to all models and model
coupling frameworks. This is known as the \textit{Parallel Coupling Problem} or
\textit{M-by-N problem}. Larson et al.\ \cite{Larson2005,Jacob2005} discuss this
in detail with respect to the creation of the Model Coupling Toolkit. Consider a
coupled system where a model is running on M processes and another model is
running on N processes. The \textit{M-by-N problem} is the problem of
transferring data between the M process model and the N process model. The
communication pattern required changes for different values for M and N. This
problem increases in complexity when more than two models are interacting with
each other. It is up to the model coupling framework to work out an efficient
and scalable solution for all possible communication patterns.

Each of the listed model couplers have many similarities however they also have
a number of differences in terms of how they couple models and the tools
offered. A comparison of model couplers is given below.

\subsubsection*{Comparison of Model Couplers}

The Model Coupling Toolkit (MCT) \cite{Larson2005,Jacob2005} is a simple
coupling framework which has been very successful having been used in the
Coupled Ocean-Atmosphere-Wave-Sediment Transport (COAWST) modelling system,
COAMPS for coastal atmosphere simulation, and WRF. The toolkit also forms the
underlying code for the coupler in the Community Climate System Model (CCSM).
The project is suffering from bitrot, with the last commit to the MCT repository
in December 2012. However, the OASIS team have integrated MCT code with their
latest framework, OASIS3-MCT so the framework is still in active use. For these
reasons MCT will not be considered in its own right, instead considered with
respect to OASIS3-MCT which is in active development.

The remaining three coupling frameworks have been looked at in detail. Overall
all frameworks are functionally similar however there are different approaches
in how coupling is expressed. ESMF has a dedicated coupler component expressed
in Fortran with explicit API calls in each model, the coupler, and master thread
for the coupling setup and data transfer. OASIS3-MCT and OpenPALM use a plain
text file to express the coupling in a much simpler, condensed format.
OASIS3-MCT allows changes to be made to the coupling configuration without
requiring recompilation whereas OpenPALM may require recompilation due to
changes in the GUI-created `glue code' for the coupling.

\textbf{ESMF} is in active development with an extensive API. The API is the
largest out of the three (the reference documentation is 1140 pages long, an
order of magnitude larger than the rest) and, since everything is done in
Fortran, the framework suffers from a significant amount of boilerplate code
that the user must write. The code is fairly straightforward so a tool could
probably be written to create most of the code automatically however such a tool
does not exist. The everything-in-code method also means recompilation is
required for any modification to the coupling configuration. On a positive note,
the extensive and granular API makes the framework arguably more flexible and
potentially easier to read since everything is in Fortran and explicitly
defined.

A dummy coupling was created between two components
\footnote{\url{https://github.com/gordon1992/LES-WRF-MPI/tree/master/Examples/ESMF}}.
The dummy coupling required around 350 source lines of code just to set up a
no-op model coupling.

Due to the seemingly unnecessary amount of code required to create a simple
coupling, ESMF was not chosen as the preferred model coupling framework to use.

\textbf{OpenPALM} hasn't had a new release since January 2013, nearly two years
ago at the time of writing. It does however seem to be in active use, with a
training session organised for January 2015. OpenPALM uses a GUI, called
PrePALM, for defining the coupling configuration. There is a small API with tens
of possible calls that need to be added to the models to be coupled. Each model
also requires specially defined Fortran comments that are `ID cards' that define
information on the model and what fields it has that can be sent to other
models. These comments are read by the GUI to help setup the coupling. The GUI
generates many files including a text file defining the coupling setup and a
number of glue code files to allow the coupling configuration to be compiled and
run. When compared with OASIS3-MCT this seems fairly complex and overall
heavyweight however the tool generates a Makefile so that the end user only
needs to interact with the GUI and doesn't need to concern themeselves with the
several files created by the GUI.

OpenPALM also offers a feature to log and automatically analyse the runtime of
the coupled system. The GUI offers views on CPU time for each model, a timeline
of what model ran when, and even a `playback' feature where the flow of
execution can be viewed graphically.

Given the reliance on the GUI, a sampling coupling was not created specifically
for comparison between ESMF and OASIS3-MCT.

Overall, due to the seemingly heavyweight approach to creating a model coupling,
OpenPALM was not chosen as the preferred model coupling framework to use.

\textbf{OASIS3-MCT} is in active development, with the latest release in May
2013. Also, from May 2014, WRF has added OASIS3-MCT coupling code
\footnote{\url{https://verc.enes.org/oasis/news/oasis3-mct-in-wrf-version-3.6}}.
This official coupling support makes using OASIS3-MCT for the LES-WRF coupling a
more attractive option.

The previous version of OASIS was OASIS3. The change from OASIS3 to OASIS3-MCT
brought a lot of changes due to the integration of MCT code, for instance the
namcouple file which contains the coupling configuration is much simpler with a
lot of content deprecated. This doesn't seem to reduce the feature set of the
coupling framework but does given the impression that OASIS could be doing with
a break in backwards compatibility for the sake of cleaning up documentation and
the framework code base.

OASIS3-MCT makes use of coupling `restart files' which are netCDF files
containing data in use. This is used to read data initially and for storing data
to be sent to another model if the model doesn't need it yet. This reliance on
I/O operations is a potential performance concern which will need to be
investigated during runtime. On the other hand, Polcher et al.\
\cite{Polcher2013} did not find I/O to be a bottleneck in their ORCHIDEE
coupling when executed on a cluster with up to 20 CPUs.

The next version of OASIS3-MCT will bring a GUI for creating the namcouple file
automatically and a performance analysis tool, LUCIA \cite{Maisonnave} which may
bring its feature set more inline with OpenPALMs offerings.

A dummy coupling was created between two components
\footnote{\url{https://github.com/gordon1992/LES-WRF-MPI/tree/master/Examples/OASIS3-MCT}}.
The coupling required an additional 100 sources lines of code (including the
plain text namcouple file) to create a coupling that transferred a 2D array
between two models.

Given ESMFs simple namcouple file format and active development, it was chosen
as the preferred model coupling framework to use.

\section*{Proposed Approach}

As stated before, there are a number of disinct stages to the project and each
stage requires a different approach.

The first stage involves creating a distributed version of the LES. This will
use MPI since it is the \textit{de facto} standard for cross-process
communication and OASIS3-MCT uses MPI1 capabilities for the coupling itself.
Even though OASIS3-MCT itself is limited to MPI1 capabilities, there should be
no issue using MPI2, or even MPI3 capabilities for the model-local parallelism
since MPI is mostly backward compatible. The dummy OASIS3-MCT coupling created
as part of the feasibility study was compiled with MPICH 3.1.2, an MPI3 capable
library without issue.

The second stage involves coupling LES with WRF. For each timestep of WRF, LES
will run about one hundred times then data is exchanged. This will need to be
expressed in the coupling mechanism to avoid unnecessary data transfers and
minimal requirement for I/O operations. There is also potential problems with
modifying WRF for the OASIS coupling. WRF is a highly complex piece of software
and making changes, especially given my lack of experience with the code base,
could be challenging.

\section*{Work Plan}

The project has a number of distinct stages with deliverables at the end of each
stage. The following major stages have been identified with the estimated
delivery date given in bold:

\begin{description}
	\item[25th November] Create distributed LES using MPI
	\item[15th December] Couple LES and WRF using OASIS3-MCT
	\item[3rd February] Evaluate performance of different node count and node
    configurations for:
    \begin{itemize}
        \item MPI LES (using multiple MPI libraries if available)
        \item Coupled System
    \end{itemize}
	\item[10th March] Compare coupling framework to GCMF
	\item[31st March] Evaluate differences in performance between OASIS3-MCT
    coupling and GCMF Coupling
    \item[24th April (Estimated)] Final deliverable in the form of a paper and a
    presentation or poster describing work completed.
\end{description}

\bibliographystyle{acm}
\bibliography{1002536r}

\end{document}
