\documentclass{acm_proc_article-sp}

\usepackage{hyperref}

\title{Exploring coupling the distributed Large Eddy Simulation and Weather
Research and Forecasting model}

\numberofauthors{1}

\author{
    \alignauthor
    Gordon Reid\\
    \affaddr{School of Computing Science}\\
    \affaddr{University of Glasgow}\\
    \email{1002536r@student.gla.ac.uk}
}

\makeatletter
\let\@copyrightspace\relax
\makeatother

\begin{document}

\maketitle

\begin{abstract}

Scientists who study the geosciences have created software that models different
aspects of our planet's ecosystem; for example, ocean, wind, land, and
atmosphere. Each model is independently useful however for more accurate results
and to model more complex aspects of our climate, these models need to be
coupled together. The project proposed involves creating a distributed version
of the Large Eddy Simulator (LES) then coupling the LES with the Weather
Research and Forecasting model (WRF) using a model coupling framework. The
coupled system should scale to large computing clusters. Ideally the coupling
would also be generic such that the OpenCL-accelerated LES variant could be used
to allow greater performance on heterogeneous systems.

\end{abstract}

\section*{Introduction}

Our planet's ecosystem is highly complex. Geoscientists that are interested in
modelling this ecosystem have created numerous systems which each model one
aspect, for instance a Large Eddy Simulator (LES) to study turbulent air flows
\cite{Nakayama2011,Nakayama2012} and the Weather Research and Forecasting Model
(WRF) for mesoscale weather prediction \cite{Michalakes2000}. For the last
decade or so, interest has evolved from individual models to combining models
\cite{Michalakes2010} to obtain more accurate results and to model more complex
aspects of our planet's ecosystem. With this trend, a number of model coupling
frameworks have been created to support the desire for co-simulation. These
model couplers allow a single application to make use of multiple models without
significant changes to the individual models' code bases. Models communicate
with each other via data exchanges at each timestep which are handled by the
coupling framework. The coupling frameworks typically use the Message Passing
Interface (MPI) for communication since MPI is the \textit{de facto} standard
for parallel communication.

The proposed work for the project involves three distinct stages. The first
requires the modification of the existing LES code to create a distributed
version that makes use of MPI to allow performance to scale on large computer
clusters. An additional aim of the first stage is to facilitate the use of the
OpenCL-accelerated LES \cite{Vanderbauwhede2014} in the distributed system to
improve the model's throughput further on heterogeneous systems. The second
stage involves coupling the distributed LES with WRF using a model coupling
framework. The third stage involves comparing the coupling from the second stage
to a coupling using the Glasgow Model Coupling Framework (GMCF)
\cite{Vanderbauwhede2014}.

A number of modeling frameworks have been looked at, including: the Model
Coupling Toolkit (MCT) \cite{Larson2005,Jacob2005}; OASIS (specifically
OASIS3-MCT) \cite{Valcke,Valcke2013}; the Earth System Modeling Framework (ESMF)
\cite{Ramework2004}; and OpenPALM \cite{Piacentini2011}. After comparing these
model coupling frameworks, OASIS3-MCT was chosen as the preferred framework
given its continued development and simple coupling mechanism.

The remainder of this document discusses the research problem in detail before
going into an in-depth literature review for the state of the art in distributed
and coupled applications. The proposed approach and work plan for the project is
then given to conclude the proposal.

\section*{Statement of Problem}

Individuals models for ocean, land, atmosphere, etc are useful in ther own right
however geoscientists are now wanting to model more complex scenarios that
require collaboration from multiple models. This collaboration can also lead to
more accurate results since additional effects from other models can be added to
the overall system. For example, coupling WRF and LES led to greater wind
velocity being predicted compared with running WRF on its own
\cite{Kinbara2010,Nakayama1998}.

Currently, WRF and LES are independent models and LES has two variants: the
original single threaded code and the OpenCL accelerated variant
\cite{Vanderbauwhede2014}. Both LES variants can only make use of the resources
of a single node. Vanderbauwhede and Takemi \cite{Vanderbauwhede2013} have also
investigated the benefits of GPU accelerating WRF and found this to be
``feasible and worthwhile'' however this is out of scope for this proposal.

The problem to be addressed is two-fold: creating a MPI variant of LES to allow
LES to run on multiple nodes in a computer cluster and then coupling this
variant of LES with WRF. The MPI variant of the LES will increase the
performance of the simulator since it will allow it to make use of a multi-node
distributed memory system such as a Beowulf cluster. The coupling of MPI LES
with WRF will create a system that benefits from scalable performance and high
accuracy results.

\section*{Literature Review}

Previous work falls under two related but largely independent areas: distributed
computing using MPI for data sharing via message passing, and model coupling.
Model couplers generally use MPI for cross-model communication however this is
generally hidden to the user under an API offered by the model coupling
framework. MPI specific work will be discussed followed by a section dedicated
to related work on model coupling.

\subsection*{Message Passing Interface}

Message Passing Interface (MPI) is a recognised standard for sharing data
between processes on parallel systems by supporting message passing between
nodes in a computer cluster or supercomputer. The standard creates a common
interface that allows any vendor to create their own implementation and ensures
that different implementations of the same version of the standard can be
interchanged without requiring user code changes for the most part. This has led
MPI to be viewed as the \textit{de facto} standard for inter-process
communication, especially across multiple nodes. There are implementations of
the standard for many languages including C, Fortran, Java, and Python.

\subsubsection*{Multithreaded MPI Communication}

In addition to computer systems making use of multiple machines, each individual
node now has multiple CPU cores and sometimes even multiple CPU sockets. This
poses an additional problem for application developers: how to enable the use of
MPI inside a multithreaded application? Multiple threads and MPI individually
bring their own complexity that a developer has to deal with however there is an
additional challenge when considering how multiple threads make MPI calls.

D\'{o}zsa and Kuma et al.\ \cite{Kumar} use multichannel-enabled network
hardware, granular locking, atomic operations, and concurrency-aware message
queues to demonstrate that multithreaded applications can have a message passing
rate that scales with multiple threads. The authors send zero-byte messages to
measure the number of messages that can be sent per second as the number of
threads grows. There is a near-linear increase in performance as the number of
threads is increased whereas the default implementation sees a message rate
\textit{decrease} as thread count increases. Performance is still worse than the
Deep Computing Message Framework (DCMF) which is a lower level messaging library
and has a message rate that scales almost linearly with the number of threads
and has performance nearly seven times greater than the optimised MPI
implementation when both run on four threads. The authors haven't considered the
latency of messages or raw throughput since there may be an effect of the size
of messages in a threaded environment and zero-byte messages are unrealistic.
The authors have only investigated message rate for a small range of threads (1
to 4) however there can be single nodes with 64 cores (16 core CPUs on a
quad-socket motherboard) or more which is where any multithreading-related
performance problems would come to light.

Thakur and Gropp \cite{Thakur2009} create and use a number of performance
benchmarks using scenarios claimed to be close to typical applications. They
test the cost of thread safety, the ability for concurrent progress to be made,
and the possibilities for computation overlap. The authors' first test involves
a simple ping-pong latency test of a single threaded application that sets up
the MPI environment to allow multiple threads to make MPI calls without having
to explicitly synchronise with the other threads. The test shows negligible
latency on some MPI implementations. The validity of this test can be questioned
because, even though the MPI library is told to expect multiple threads making
calls simultaneously, this doesn't happen since the processes remain single
threaded. This means that the effect of any locks etc used by the MPI library
may not be being appropriately exercised to determine the overhead. The authors'
second test investigated cumulative bandwidth of MPICH2 and Open MPI
implementations, comparing each library's performance where parallelism is
achieved via processes or threads. For the Linux cluster the 1Gbit/s link
between nodes is saturated in all bar one case and thus no conclusions can be
made. For the Sun and IBM cases, threading is shown to dramatically decrease
throughput (over 50\%) in the threaded case compared to the process parallel
case. The bandwidth in Sun's multithreaded case was less than the saturated
bandwidth multithreaded case for the Linux cluster so this appears to show that
Sun can significantly improve the performance of their MPI implementation.

The other tests overall show how MPICH2 and Open MPI have fairly good
performance however the authors note that, due to the Gigabit Ethernet
interconnect, the overheads are generally masked since the Ethernet connection
is saturated in all cases, as shown by the cumulative bandwidth test. The paper
has a more comprehensive look into the performance differences of MPI libraries
and shows that significant work is required to minimise the overhead caused by
multithreaded MPI applications. The authors also make their benchmark test suite
available online for others to repeat their tests and see how different hardware
setups react and how later versions of MPI libraries improve.

\subsection*{Model Coupling}

Applications currently exist that model a single aspect of our Earth's
ecosystem. Each model is useful in their own right however more accurate and
detailed analysis can be conducted by making use of multiple models. This isn't
viable by using the models independently so there is significant motivation to
construct an efficient method of allowing applications to interact.

There are three ways applications can be coupled together \cite{Thevenin}. The
first involves merging the two code bases. This solution is very efficient and
portable as a single process is created. Data is shared via memory exchanges
which are lightweight however the process of merging is complicated and the two
code bases are no longer independent making continued development of each model
difficult. Merging also causes problems for parallelism since each model's
parallel abilities are dictated by the overarching code that controls both
models. For these reasons this solution is not an appropriate way to couple
models.

The second solution \cite{Thevenin} is to use a communication library directly,
such as MPI. This allows each model to be kept independent with the only code
changes involved being related to the communication between models. The solution
still suffers from many problems. The coupling code isn't generic since each
model needs to know exactly what model it is sending data to. Also, since the
MPI API is fairly low level, knowledge of parallel computing techniques is
required for efficient coupling. The problems of complexity and generality
increase if there are many models involved in the coupling with complicated
exchanges required. Using MPI for data exchanges is definitely a viable way
forward however a higher level solution is required to make coupling many models
viable.

The third solution \cite{Thevenin} follows on from the second. Instead of using
an MPI library directly, a coupling library is employed to abstract the low
level complexities of coupling away from the developer. A coupling library is a
separate piece of software that acts as the ``middle man'' between models and
facilitates data exchanges between models using a framework-specific interface.
Coupling libraries can also provide tools for interpolation such as regridding
\cite{Kuinghttons} if there are differences in data representation between
communicating models. Some coupling libraries also supply tools for performance
analysis with graphical representations of runtime execution characteristics.
There are many examples of coupling libraries in use, for example: the Model
Coupling Toolkit (MCT) \cite{Larson2005,Jacob2005}; OASIS3-MCT
\cite{Valcke,Valcke2013}; the Earth System Modeling Framework (ESMF)
\cite{Ramework2004} and OpenPALM \cite{Piacentini2011}.

There are two main types of coupling: static and dynamic
\cite{Thevenin,Piacentini2011}. In static coupling, all of the coupled
components have to start simultaneously at the beginning of the simulation. This
means any memory and CPU resources required by each model are allocated and
locked until the end of the simulation, even if a model doesn't start using the
resources until midway through the simulation. Dynamic coupling allows each
coupled component to start and end as required, freeing up resources when not in
use. Instead, resources are managed by the coupler. Generally there is a fixed
amount of resources available to the coupler at the beginning of the simulation
however the coupler is free to dynamically manage how the resources are used to
meet the requirements of the currently running components.

During runtime, the models themselves can either be run sequentially or
concurrently \cite{Maisonnave}. If two models need the results of each other
from a previous timestep, they can run concurrently. If one of the models needs
the result of the other from the current time step, the models run sequentially.
If more than two models are coupled together, both sequential and concurrent
execution may be required.

Coupling parallel models has a problem that is generic to all models and model
coupling frameworks. This is known as the \textit{Parallel Coupling Problem} or
the \textit{M-by-N problem}. Larson et al.\ \cite{Larson2005,Jacob2005} discuss
this in detail with respect to the creation of the Model Coupling Toolkit.
Consider a coupled system where a model is running on M processes and another
model is running on N processes. The \textit{M-by-N problem} is the problem of
transferring data between the M process model and the N process model. The
communication pattern required changes for different values for M and N. This
problem increases in complexity when more than two models are interacting with
each other. It is up to the model coupling framework to work out an efficient
and scalable solution for all possible communication patterns.

Each of the listed model couplers have many similarities however they also have
a number of differences in terms of how they couple models and the tools
offered. A comparison of model couplers is given below.

\subsubsection*{Model Couplers}

The Model Coupling Toolkit (MCT) \cite{Larson2005,Jacob2005} is a simple
coupling framework which has been very successful. It has been used to couple
many models, for example the Coupled Ocean Atmosphere Wave Sediment Transport
(COAWST) modelling system, COAMPS for coastal atmosphere simulation, and WRF.
The toolkit also forms the underlying code for the coupler in the Community
Climate System Model (CCSM). The project is suffering from bitrot, with the last
commit to the MCT repository in December 2012. However, the OASIS team have
integrated MCT code with their latest framework, OASIS3-MCT, so the framework is
still in active use. For these reasons MCT will not be considered in its own
right, instead considered with respect to OASIS3-MCT which is in active
development.

The remaining three coupling frameworks have been looked at in detail. Overall
all frameworks are functionally similar however they employ different approaches
in how coupling is expressed and behaves. ESMF has a dedicated coupler component
expressed in Fortran with explicit API calls in each model, the coupler, and
master thread for the coupling setup and data transfer. OASIS3-MCT and OpenPALM
use a plain text file to express the coupling in a much simpler, condensed
format however OpenPALM generates `glue code' to create a master process whereas
OASIS3-MCT requires no extra process. OASIS3-MCT allows changes to be made to
the coupling configuration without requiring recompilation whereas OpenPALM may
require recompilation due to changes in the GUI-created `glue code' for the
coupling.

\textbf{ESMF} is in active development with an extensive API. The API is the
largest out of the three (the reference documentation is 1140 pages long, an
order of magnitude larger than the rest) and, since everything is done in
Fortran, the framework suffers from a significant amount of boilerplate code
that the user must write. The code is fairly straightforward so a tool could
probably be written to create most of the code automatically however such a tool
does not exist. The everything-in-code method also means recompilation is
required for any modification to the coupling configuration. On a positive note,
the extensive and granular API makes the framework arguably more flexible and
potentially easier to read since everything is in Fortran and explicitly
defined.

\textbf{OpenPALM} hasn't had a new release since January 2013, nearly two years
ago at the time of writing. OpenPALM uses a GUI, called PrePALM, for defining
the coupling configuration. There is an API with tens of possible calls that
need to be added to the models to be coupled. Each model also requires specially
defined Fortran comments that are `ID cards' that define information on the
model and what fields it has that can be sent to other models. These comments
are read by the GUI to help setup the coupling. The GUI generates many files
including a text file defining the coupling setup and a number of glue code
files to allow the coupling configuration to be compiled and run. When compared
with OASIS3-MCT this seems fairly complex and overall heavyweight however the
tool generates a Makefile so that the end user only needs to interact with the
GUI and doesn't need to look at the several files created by the GUI.

OpenPALM also offers a feature to log and automatically analyse the runtime of
the coupled system. The GUI offers views on CPU time for each model, a timeline
of what model ran when, and even a `playback' feature where the flow of
execution can be viewed graphically.

\textbf{OASIS3-MCT} is in active development, with the latest release in May
2013. Also, from May 2014, WRF has added OASIS3-MCT coupling code in its version
3.6 release \cite{ENES2014}. This official coupling support makes using
OASIS3-MCT for the LES-WRF coupling a more attractive option.

The previous version of OASIS was OASIS3. The move from OASIS3 to OASIS3-MCT
brought a lot of changes due to the integration of MCT code, for instance the
`namcouple' file which contains the coupling configuration is much simpler with
a lot of content deprecated. This doesn't seem to reduce the feature set of the
coupling framework but does given the impression that the OASIS framework could
benefit from a clean up of documentation and the framework code base.

Depending on the coupling configuration, OASIS3-MCT may require the use of
coupling `restart files' which are netCDF \cite{Unidata} files containing data
in use. This is used to read data initially and for storing data to be sent to
another model if the recipient model doesn't need it yet. This reliance on I/O
operations is a potential performance concern which will need to be investigated
during runtime. On the other hand, Polcher et al.\ \cite{Polcher2013} did not
find I/O to be a bottleneck in their ORCHIDEE coupling when executed on a
cluster with up to 20 CPUs.

The next version of OASIS3-MCT will bring a GUI for creating the namcouple file
automatically and a performance analysis tool, LUCIA \cite{Maisonnave}, which
may bring its feature set more inline with OpenPALM's offerings.

\subsubsection*{Comparison of Model Couplers}

Each model coupler takes a different approach to the overall coupling
architecture and communication strategies.

ESMF has two layers, `superstructure' and `infrastructure' which sandwich the
user code being coupled. The superstructure is the upper layer which provides
high level components to the developer, such as Gridded Components to associate
with each model, Coupler Components to define how two models interact, and
States to encapsulate data between models. The infrastructure layer is the lower
layer which provides tools to manage the complexity of the different time steps
and data representations between models. The architecture is designed such that
each ESMF component doesn't need to know anything about the models it is coupled
with allowing component reuse and limited changes to the model code itself. The
design is also very flexible, allowing components to be coupled in any manner
required, for instance a hub-and-spokes coupling, and does not restrict modes of
execution to just solely sequential or solely concurrent.

For communication, ESMF is also very flexible, even allowing model
communications to occur in the middle of a time step. There is no communication
mechanism hardwired into ESMF and, since all communication between models is
done via an explicit coupling component, any number of transformations can be
executed on data before being given to the receiving model. ESMF has a ``Uniform
communication API'' \cite{ESMF2014} meaning the same interface is used for
shared memory and distributed memory communication. This interface is realised
by the ESMF Virtual Machine, an abstraction of machine architecture, which
handles all of the resource management and communication methods. For load
balancing, ESMF supports load balancing packages such as Parmetis
\cite{Hoefler2010,Karypis1998} or developer-defined methods of load balancing.

OpenPALM has a simpler architecture to ESMF and a lot of the underlying
complexity is hidden by the supplied graphical user interface, PrePALM. There is
still an explicit coupler component however PrePALM generates the required glue
code and Makefile automatically. As a result, after compilation, a `palm\_main'
executable is created alongside the model executables. This is executed directly
and it manages the execution of the model executables as defined in the GUI.

For communication, OpenPALM uses an ``end point'' communication methodology
\cite{CERFACS2007} and components are kept independent from each other. This
means that when a model produces data that it would like to make available to a
component, it executes a non-blocking `PALM\_Put' call. If another model
requires data at some point, it executes a blocking `PALM\_Get' call. At the
model level, there is no need for the calls to match up with another model, it
is only when the coupling is being created are the get and put calls matched up.
The primary benefit of this is that if a model is being reused in another
coupling, it is unlikely that many of the calls themselves will need to be
modified, just the configuration in the GUI. Another benefit is that this also
means data transfer doesn't need to happen straight away, the recipient could be
executed later or may be in the middle of some computation so the put call is
non-blocking and the data is buffered, awaiting the corresponding get call.

OpenPALM works around the \textit{M-by-N problem} in a manner largely
transparent to the developer. For each model with multiple processes, OpenPALM
is aware of what process contains what portion of each array so, when processes
make get/put calls, it works out which processes need to communicate with each
other to complete the calls. The developer is not required to bundle together
the array to be sent nor does the library do this.

OASIS3-MCT's architecture is simpler still \cite{OASIS3-MCT2013}. OASIS3-MCT is
only a library just like ESMF and OpenPALM however, thanks to MCT, it requires
no explicit coupling components as with ESMF and OpenPALM. Any transformations
required for communication between models is executed on either the sender or
recipient processes since there is no central coupler executable. This also
necessitates point-to-point communication between models, similar to OpenPALM.
OASIS3-MCT's method of dealing with the \textit{M-by-N problem} is the same as
OpenPALM and `put' calls are also non-blocking with `get' calls blocking.
OASIS3-MCT also allows buffering of data through the use of coupler restart
files stored on disk during runtime.

To get a feel for how each framework is used and behaves, a coupling was created
using all three frameworks.

A dummy coupling was created between two components in ESMF
\footnote{\url{https://github.com/gordon1992/LES-WRF-MPI/tree/master/
Examples/ESMF}}. The dummy coupling required around 350 source lines of code
just to set up a `no-op' model coupling. Due to the overly large amount of code
required to create a simple coupling, ESMF was not chosen as the preferred model
coupling framework to use.

For OpenPALM, the reliance on the GUI meant that a sampling coupling was not
created specifically for comparison between ESMF and OASIS3-MCT in terms of
required code changes, instead solely for getting a feel for the framework.
Overall, due to the seemingly heavyweight approach to creating a model coupling,
OpenPALM was not chosen as the preferred model coupling framework to use.

A dummy coupling was created between two components in OASIS3-MCT
\footnote{\url{https://github.com/gordon1992/LES-WRF-MPI/tree/master/
Examples/OASIS3-MCT}}. The coupling required an additional 100 sources lines of
code (including the plain text namcouple file) to create a coupling that
transferred a 2D array between two models.

Given OASIS3-MCTs simple namcouple file format and active development, it was
chosen as the preferred model coupling framework to use.

\section*{Proposed Approach}

As stated before, there are a number of distinct stages to the project and each
stage requires a different approach.

The first stage involves creating a distributed version of the LES. This will
use MPI since it is the \textit{de facto} standard for cross-process
communication and OASIS3-MCT uses MPI1 capabilities for the coupling itself.
Even though OASIS3-MCT itself is limited to MPI1 capabilities, there should be
no issue using MPI2, or even MPI3 capabilities for the model-local parallelism
since MPI is mostly backward compatible. The dummy OASIS3-MCT coupling created
as part of the feasibility study was compiled with MPICH 3.1.2, an MPI3 capable
library, without issue.

The second stage involves coupling LES with WRF. For each timestep of WRF, LES
will run about one hundred times before data is exchanged. This will need to be
expressed in the coupling mechanism to avoid unnecessary data transfers and
minimal requirement for I/O operations. There are also potential problems with
modifying WRF for the OASIS coupling since WRF is a highly complex piece of
software and thus making changes, especially with a lack of experience with the
code base, could be challenging.

The evaluation of performance will have two parts: evaluating the MPI LES and
the coupled system performance as node count increases, perhaps with different
MPI implementations (say MPICH and OpenMPI). MPI libraries aren't strictly
interchangeable however with the availability of the wrapper compiler
\textit{mpifort} mixing MPI implementations may be straightforward.

The third stage involves comparing the differences in performance of the
OASIS3-MCT coupling to the GMCF coupling. GMCF is in a proof-of-concept stage
however has already been used to couple OpenMP WRF with the OpenCL accelerated
LES variant successfully.

\section*{Work Plan}

The project has a number of distinct stages with deliverables at the end of each
stage. The following major deliverables have been identified with the estimated
delivery date given in bold:

\begin{description}
	\item[2nd December] Distributed LES implementation
	\item[19th December] Coupled system of LES and WRF using OASIS3-MCT
	\item[3rd February] Performance evaluation for:
    \begin{itemize}
        \item MPI LES (using multiple MPI libraries if available)
        \item Coupled System
    \end{itemize}
	\item[31st March] Performance evaluation and qualitative comparison between
    the OASIS3-MCT coupling and the GMCF coupling
    \item[Week beginning 20th April] Final deliverable in the form of a paper
    and a presentation or poster describing work completed.
\end{description}

\bibliographystyle{IEEEannot}
\bibliography{1002536r}

\end{document}
