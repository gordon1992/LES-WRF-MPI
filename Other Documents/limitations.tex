\documentclass{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{url}

\title{Limitations}

\author{Gordon Reid - 1002536r}

\begin{document}

\maketitle

\section{LES}

\subsection{Effect of Corner Estimation in Halo Boundaries}

As shown by a Github Issues comment
\footnote{\url{https://github.com/gordon1992/LES-WRF-MPI/issues/15#issuecomment-65896162}}
the corner estimation code makes a difference to output. Taking the original LES
code and executing it on a 37x37x90 example with and without corner estimation
code resulted in differing timesteps where non-convergence occurred. This does
not suggest a place where corner estimation makes a difference but shows that it
has some effect on the data. Only a single place needs to exist where estimated
values are used to calculate another value for the difference to occur. Once
this occurs the effect of the estimation will `infect' the rest of the array,
radiating out from the estimated corner. This was found in netCDF output for
pressure in the MPI version where there was a slightly different value for
pressure compared to the original non-estimated code radiating out from corners
of the array.

Places where corner estimated values are used:

\begin{itemize}
    \item les.f95\\
    ! --calculation of sgs eddy viscosity coeficient\\
    - dudyx1 = ... diu2(i-1,j+1,k) ... - Top Right\\
    - dvdxx1 = ... diu4(i+1,j-1,k) ... - Bottom Left\\
    These are then used to calculate values for sm\\
    sm is then used to calculate values for eddyviscosity values\\
    eddyviscosity values are then used to calculate values for f, g, and h\\
    and so on and so forth, called every timestep.
    \item vel2.f95\\
    - nou2(i,j,k) = ... v(i+1,j-1,k) ... - Bottom Left \\
    Used to calculate cov2
    \item vel2\_init.f95\\
    - nou2(i,j,k) = ... v(i+1,j-1,k) ... - Bottom Left \\
    Used to calculate cov2\\
    Doesn't seem to be called however.
\end{itemize}

Exact corner code exists. Sending exact corners does not seem to negatively
affect performance and has been seen to marginally improve performance over the
estimated corner case. With corner estimation requiring three floating point
operations per float (two additions and a division), then it isn't unsurprising
that sending floats instead isn't expensive to do in comparison.

\section{GMCF}

\subsection{Workarounds and Hacks}

With GMCF being a work in progress there are a number of desired features that
are not currently implemented and a number of workarounds and hacks including
hard coded values that should be worked out at compile-time instead.

\begin{itemize}

    \item The td files for defining each model in the coupling and the model's
    unique ID are manually created. Currently a Python script exists to generate
    this for simple examples.

    \item The yml files for defining model communication, method names etc are
    manually created instead of being defined from the td file and the model
    code. Currently a Python script exists to generate this for simple examples.

    \item The gmcfConnectivityMatrix is currently hard coded instead of being
    calculated based on the contents of the yml file. Currently a Python script
    exists to generate this for simple examples. The gmcfConnectivityMatrix is
    unused bar FIN packets at the moment anyway.

\end{itemize}

\subsection{Performance}

\begin{itemize}

    \item There is a double free/memory corruption crash when leaving the model
    subroutine. This is probably something to do with Fortran silently doing
    allocates for stack variables and freeing them, then C++ freeing them or
    something like that.

    \item For large messages (100K reals), MPI has 3x the bandwidth than GMCF
    using the packets method.

    \item For small messages (1 real), MPI has 1000x the bandwidth than GMCF
    using the packets method.

    \item For a small number of threads (say 4), GMCF is slightly faster/on-par
    with MPI for global reductions (sum, min, max).

    \item For a large number of threads (say 64), GMCF is 10x slower than MPI
    for global reductions but is still 100x faster than the packets method for
    doing the same thing.

\end{itemize}

\subsection{Future improvements}

\begin{itemize}

    \item Make use of type(*) in Fortran 2003 to provide generic send and
    receive functions, rather than gmcfSendReal4Array, etc.

    \item Abstracting away the notion of packets into subroutines to minimise
    the code the end user needs to write.

    \item Change the global reduction to a tree-based reduction, similar to
    OpenMPI's method. This won't improve performance significantly for tens of
    threads but will probably help scaling to the hundreds of threads.

    \item Add MPI to GMCF for distributed-memory parallelism. (This may mean
    global reductions in GMCF could just use MPI for sake of simplifying
    framework code).

\end{itemize}

\end{document}
